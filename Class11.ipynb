{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonathan-Nyquist/PLAM/blob/main/Class11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuL4Rqo7jjt1"
      },
      "source": [
        "<a id='Top'> </a>\n",
        "\n",
        "# Class 11: Advanced Data Analysis\n",
        "\n",
        "## Learning Objectives:\n",
        "\n",
        "Once again, the goal of the notebooks in this part is to open your eyes to the possibilities and inspire you to continue to teach yourself Python on your own. Do not worry if you don't understand all of the data analysis techniques that are demonsrated here.\n",
        "\n",
        "In the previous notebook we looked at some advanced ways of obtaining data. In this notebook we'll look at a few advanced data analysis techniques. In the final notebook we'll look at advanced data presentation.\n",
        "\n",
        "In this class we will look at:\n",
        "- [Statistics and Correlation](#Stats)\n",
        "- [Image Processing](#Images)\n",
        "- [Natural Language Processing](#Language)\n",
        "- [Machine Learning](#Machine)\n",
        "- [Student Challenge 1](#Student1)\n",
        "- [Student Challenge 2](#Student2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6NSVH4Hjjt3"
      },
      "source": [
        "<a id='Stats'></a>\n",
        "## Statistics and Correlation\n",
        "[Top of Notebook](#Top)\n",
        "\n",
        "Just for fun, and to illustrate the idea of spurious correlations, I asked students in one of my \"Evil Plots\" class (another General Education class I created) the following set of random questions:\n",
        "\n",
        "1.\tHow many hours a week do you spend on school assignments and studying?\n",
        "2.\tHow much do you love math on a scale of 1-10 (1=would rather have my teeth drilled, 10=math problems are better than ice cream and kittens)\n",
        "3.\tWhat day of the month were you born?\n",
        "4.\tWhat is your height in inches to the nearest inch?\n",
        "5.\tHow many days did you spend at the beach this year?\n",
        "6.\tWhat is the most miles you‚Äôve driven a car in a single day, ever?\n",
        "7.\tHow many songs do you listen to in a day?\n",
        "8.\tHow many slices of pizza did you eat in the past month (best estimate).\n",
        "9.\tHow many states have you visited in your life? (Driving though or stopped in an airport count)\n",
        "10.\tHow many letters are in your first and last name combined?\n",
        "\n",
        "*** Let's load the data and look for correlations! ***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Jonathan-Nyquist/PLAM/raw/main/RandomData.xlsx"
      ],
      "metadata": {
        "id": "8d-3F-ddkFbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "joAo6WRajjt4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data =  pd.read_excel('RandomData.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vsBmzcLjjt5"
      },
      "source": [
        "I learned two things just looking at the data:\n",
        "1. Some students eat just an incredible amount of pizza!\n",
        "2. There are always a few who can't follow directions. When asked what day of the month they were born, they gave me the month of the year. So I had to enter NaN (not-a-number) values.\n",
        "\n",
        "There is no reason to believe any of these data should be correlated. I mean, pizza consumption and letters in your name... Seriously? But let's take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IqrjVm0jjjt5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Extract columns\n",
        "x = data['PizzaSlices']\n",
        "y = data['LettersInName']\n",
        "\n",
        "# Fit a line to the data\n",
        "# numpy has a function that fits polynomials and a line is just a first degree polynomial.\n",
        "fit = np.polyfit(x, y, deg=1)\n",
        "print(fit)\n",
        "\n",
        "# Create figure, plot line and data\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, fit[0] * x + fit[1], color='red')\n",
        "ax.scatter(x, y)\n",
        "plt.xlabel('Pizza Slices Consumed Per Month')\n",
        "plt.ylabel('Combined Number of Letters in Student Name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4aYDgAXjjt5"
      },
      "source": [
        "No surprise here; it is not a very good correlation.  We could plot all the combinations against each other one at a time, but that would be tedious. Pandas to the rescue! (Again, don't worry about a few of the advanced tricks in the code.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "R9ZjrOX3jjt5"
      },
      "outputs": [],
      "source": [
        "# Create a matrix of scatter plots by looping over all the columns\n",
        "# We import a package that will do the hard work for us.\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "axes = scatter_matrix(data, diagonal='hist', figsize=(12,12))\n",
        "corr = data.corr().to_numpy()\n",
        "\n",
        "# This bit is tricky, but we are looping over all the combinations\n",
        "for i, j in zip(*plt.np.triu_indices_from(axes, k=1)):\n",
        "    axes[i, j].annotate(\"%.3f\" %corr[i,j], (0.8, 0.8), xycoords='axes fraction', ha='center', va='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w11WTktjjt6"
      },
      "source": [
        "1 = perfect positive correlation, 0 = no correlation at all, -1 = perfect negative correlation.\n",
        "\n",
        "Most of the correlation coeffecients are very low, as expected.  The strongest correlation is 0.65.  Let's take a closer look at that one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bF4bHqBUjjt6"
      },
      "outputs": [],
      "source": [
        "# Extract columns\n",
        "x = data['PizzaSlices']\n",
        "y = data['BeachDays']\n",
        "\n",
        "# Fit a line to the data\n",
        "fit = np.polyfit(x, y, deg=1)\n",
        "\n",
        "# Create figure, plot line and data\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x, fit[0] * x + fit[1], color='red')\n",
        "ax.scatter(x, y)\n",
        "plt.xlabel('Pizza Slices Consumed Per Month')\n",
        "plt.ylabel('Days Spent At The Beach This Summer')\n",
        "\n",
        "#data.plot.scatter('PizzaSlices', 'LettersInName')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owYb32bMjjt6"
      },
      "source": [
        "This is when your brain starts thinking, \"Hmmm.  Maybe people who eat a lot of pizza are the type who laze around on the beach all summer.\" Well, maybe...  More likely, this is just a spurious correlation. If you plot enough things against each other you're bound to find some that correlate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjZCOU28jjt6"
      },
      "source": [
        "### Student Challenge\n",
        "Here is a really fun book on [spurious correlations](http://tylervigen.com/spurious-correlations). Click on the link and look at a few of the examples. Comment on your favorite in the markdown cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bry_WZnojjt6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCr-p89Ejjt6"
      },
      "source": [
        "But I digress.  The point was that we just did a pretty sophisticated data analysis of all of the possible pair-wise correlations between ten different variables with only a few lines of python code.\n",
        "\n",
        "**Whether you're a scientist analysing experimental results, a business analyst summarizing stock trends, or a sociologist evaluating risk factors for children, being able to power through your data like this gives you a superpowers beyond the imagination of puny mortals! With enough Python code you can take over the world.  MU HA HA HA!**\n",
        "\n",
        "(Sorry. Forgot to take my meds this morning.) üë®üèΩ‚Äçüî¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlmHRnRhjjt7"
      },
      "source": [
        "<a id='Student1'></a>\n",
        "## Student Challenge\n",
        "[Top of Notebook](#Top)\n",
        "\n",
        "Try plotting one of the other combinations, such as love of math versus number of songs listened to per day. Include the regression line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfsV4KxIjjt7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rUXpRaNjjt7"
      },
      "source": [
        "<a id='Images'></a>\n",
        "## Image Processing\n",
        "[Top of Notebook](#Top)\n",
        "\n",
        "The second form of advanced data analysis we'll explore is image analysis. Here is an image of geologic beds disrupted by faulting and folding (kind of like my bed in the morning).\n",
        "\n",
        "!['Rolled Beds'](https://pbs.twimg.com/profile_images/229857411/SanAndreas.jpg)\n",
        "\n",
        "You can see the sediment bedding, normally horizontal, has been distorted. But to extract the bedding layers from the image we can use a technique called \"edge detection.\" **Let's see if we can enhance the boundaries between the layers.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PxNb05Lmjjt7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Here we import a new module used for image processing\n",
        "from skimage import io\n",
        "\n",
        "# Load the image directly from the web using the URL (web address)\n",
        "photo = io.imread('https://pbs.twimg.com/profile_images/229857411/SanAndreas.jpg')\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(photo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "cMQSNAuCjjt7"
      },
      "outputs": [],
      "source": [
        "from skimage.color import rgb2gray\n",
        "\n",
        "# Convert the image from color to grayscale\n",
        "img_gray = rgb2gray(photo)\n",
        "plt.figure(figsize=(20,10))\n",
        "imgplot = plt.imshow(img_gray, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "0w76zORHjjt7"
      },
      "outputs": [],
      "source": [
        "# Adjust the colors by equalizing the image histogram, this increases the contrast\n",
        "# see: http://scikit-image.org/docs/dev/auto_examples/plot_equalize.html)\n",
        "\n",
        "from skimage import exposure\n",
        "\n",
        "img_eq = exposure.equalize_hist(img_gray)\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# Plot the image in black & white\n",
        "imgplot = plt.imshow(img_eq, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "yKIlXM-Kjjt7"
      },
      "outputs": [],
      "source": [
        "from skimage import feature\n",
        "\n",
        "# Use the Canny filter to find the edges.\n",
        "# http://scikit-image.org/docs/dev/auto_examples/edges/plot_canny.html#sphx-glr-auto-examples-edges-plot-canny-py\n",
        "# What this filter does is locate sudden changes from dark to light areas in the image which are typically boundaries\n",
        "\n",
        "edges = feature.canny(img_eq)\n",
        "plt.figure(figsize=(20,10))\n",
        "imgplot = plt.imshow(edges, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "CqbNZgXCjjt7"
      },
      "outputs": [],
      "source": [
        "# If we only want to see the strongest edges we can raise the threshold to only flag boundaries that\n",
        "# are serveral standard deviations above the background -- only the biggest changes.\n",
        "\n",
        "edges2 = feature.canny(img_eq, sigma=3)\n",
        "plt.figure(figsize=(20,10))\n",
        "imgplot = plt.imshow(edges2, cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1vFwR1jjt7"
      },
      "source": [
        "**Books and books have been written on image processing.  Python has many modules to help.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0nJSoqnjjt7"
      },
      "source": [
        "<a id='Language'></a>\n",
        "## Natural Language Processing\n",
        "[Top of Notebook](#Top)\n",
        "\n",
        "**A picture is worth a thousand words.** But sometimes you have to work with words not pictures. Most people know computers can be used to analyze images, but text? Yes, there is a lot of research in this areas as well, that goes way beyond just finding out how many times the word \"NASA\" appears in a newspaper article. Just as processing images is critical to problems such as computer vision and robotics, teaching a computer to process text is part of the quest for artificial intellegence. SIRI on your Iphone is a prime example - voice recognition software feeds into natural language processing for SIRI to cater to your every whim. Now, of course, we have ChatGPT, Bard and other generative AI platforms.\n",
        "\n",
        "Natural language processing is used for many tasks, such as:\n",
        "* sentiment analysis\n",
        "* spam filtering\n",
        "* plagarism detection\n",
        "* document categorization\n",
        "* phrase extraction\n",
        "* smarter searches\n",
        "* keyword analysis\n",
        "\n",
        "\n",
        "**Let's get a glimpse of natural language processing with Python.**\n",
        "\n",
        "First, we need some text to work with..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "662P0MJ0jjt8"
      },
      "outputs": [],
      "source": [
        "quote = '''LOG ENTRY: SOL 381 I‚Äôve been thinking about laws on Mars.\n",
        "\n",
        "Yeah, I know, it‚Äôs a stupid thing to think about, but I have a lot of free time.\n",
        "\n",
        "There‚Äôs an international treaty saying no country can lay claim to anything that‚Äôs not on Earth.\n",
        "And by another treaty, if you‚Äôre not in any country‚Äôs territory, maritime law applies.\n",
        "\n",
        "So Mars is ‚Äúinternational waters.‚Äù\n",
        "\n",
        "NASA is an American nonmilitary organization, and it owns the Hab. So while I‚Äôm in the Hab, American\n",
        "law applies. As soon as I step outside, I‚Äôm in international waters. Then when I get in the rover, I‚Äôm\n",
        "back to American law.\n",
        "\n",
        "Here‚Äôs the cool part: I will eventually go to Schiaparelli and commandeer the Ares 4 lander. Nobody\n",
        "explicitly gave me permission to do this, and they can‚Äôt until I‚Äôm aboard Ares 4 and operating the\n",
        "comm system. After I board Ares 4, before talking to NASA, I will take control of a craft in international\n",
        "waters without permission.\n",
        "\n",
        "That makes me a pirate!\n",
        "\n",
        "A space pirate!'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUQDPkP4jjt8"
      },
      "source": [
        "### Tokenizing\n",
        "Let's take the quote above and using Python's natural language toolkit we'll \"tokenize\" it. This breaks the quote into lists of substrings and punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3Y8wTZHyjjt8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(quote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "w364Qfz0jjt8"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FwVqxH9ojjt8"
      },
      "outputs": [],
      "source": [
        "# Break the quote apart by sentences.\n",
        "sentences = nltk.tokenize.sent_tokenize(quote)\n",
        "\n",
        "# Print each sentence with a black line in between.\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJcYgwgjjt8"
      },
      "source": [
        "You're probably not impressed. Splitting on sentences just means looking for periods, right? Not really. You have to look for question marks, exclamation points and sentences where the period is inside the quotes.  It's trickier than it sounds.\n",
        "\n",
        "But wait, there's more.  NLTK also know parts of speech. Is a given word a noun, a verb, a proposition?\n",
        "\n",
        "Here are what some of the tags mean:\n",
        "\n",
        "[![Tagging.jpg](https://i.postimg.cc/Jhd85ZXH/Tagging.jpg)](https://postimg.cc/p98329qP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Nn7TtB0pjjt8"
      },
      "outputs": [],
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tag(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEG7pmhijjt8"
      },
      "source": [
        "We will not try to go farther than this, but natural language processing is big business and also related to our next topic..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-aSTJyCjjt8"
      },
      "source": [
        "<a id='Machine'></a>\n",
        "## Machine Learning\n",
        "[Top of Notebook](#Top)\n",
        "\n",
        "Every time you shop on Amazon, or browse for movies on NetFlix, the site offers recommendations based on your previous orders or browser history. The machine is learning all about you.\n",
        "&nbsp;\n",
        "\n",
        "When you search for cute kitten pictures on Google images, how on earth does the computer know which of the millions of images uploaded to the web have cats, let alone which ones are cute? Machine learning again.\n",
        "&nbsp;\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "![Puss](https://s-media-cache-ak0.pinimg.com/originals/d3/e9/fc/d3e9fc222c9bd0d12e0ff126acf7df00.png)\n",
        "&nbsp;\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "How does your email program know to put the latesst plea for help from that Nigerian prince with the frozen bank account into your spam folder?  You guessed it. Machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzGQZF0Ejjt8"
      },
      "source": [
        "**Most applications of machine learning fall into one of three categories:**\n",
        "1. Regression - where you use data to predict something like stock trends.\n",
        "2. Supervised Classification (clustering) - where the computer learn to classsify stuff based on examples provided, such as images with a cat or no cat.\n",
        "3. Unsupervised Classification - where the computer sorts things into groups it thinks are similar in some way, such as recognizing items that tend to be purchased together.\n",
        "\n",
        "Python has a machine learning module called skikit-learn that implements many different algorithms for all three types of problems. Machine learning is a huge field, and I'm not an expert (although Temple has a few faculty members who are experts), so we will just look at a simple example to get an idea of how it works.\n",
        "\n",
        "We will consider an example of unsupervised classification. The basic goals it to sort things into categories based on a list of traits. You want things put in the same category to be more similar to each other than to things in other categories. The categories will be determined as you go along. You probably did something like this as a kid, sorting pebbles into different piles, or organizing your toys. Okay, well your mom probably did the toy thing.\n",
        "\n",
        "A common algorithm that is used is called [k-means.](https://en.wikipedia.org/wiki/K-means_clustering)\n",
        "\n",
        "**Let's apply this algorithm to the questions I asked my \"Evil Plots\" class.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "O21oWmB8jjt8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data =  pd.read_excel('RandomData.xlsx')\n",
        "plt.scatter(data['MathLove'], y=data['HoursStudying'])\n",
        "plt.xlabel('Love of Math')\n",
        "plt.ylabel('Hours/week Spent Studying')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YQ7xrInjjt8"
      },
      "source": [
        "There is clearly not much overrall correlation between student's love of math and the number of hours per week they report studying. But are there clusters? That is, are there groups of students that tend to answer the same way to both questions? To the human eye there appear to be three clusters: one group of three students clusters in the lower left corner of the graph that hates math and spents little time studying, one group that also likes math and spends little time studying, and one group that that feels so-so about math and studies a lot. There is also one oddball who really hates math but studies more than anyone else.\n",
        "\n",
        "Let's not get carried away interpreting these clusters. We only have sample size of 13 students. What we want to know is whether or not computers can spot these same clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm8laV4yjjt8"
      },
      "source": [
        "Because in a typical data set some categories (called attributes) have larges numbers (driving miles) and others have small numbers (math love), a common preprocessing step is to scale each attribute to have a similar mumeric range so that the attributes with large numbers don't dominate when we combine data. This is called \"standarizing the data.\" We subtract the average so the data will be centered on zero and divide by the standard deviation so the numbers will have similar ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Kq0ltnGCjjt8"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "data_standardized = (data - data.mean()) / data.std()\n",
        "\n",
        "plt.scatter(data_standardized['MathLove'], y=data_standardized['HoursStudying'])\n",
        "plt.xlabel('Standardized Love of Math')\n",
        "plt.ylabel('Standardized Spent Studying')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtNpEX7cjjt8"
      },
      "source": [
        "The plot looks the same, but notice that the x and y axes now have similar ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "lUZtDPtZjjuE"
      },
      "outputs": [],
      "source": [
        "data_standardized.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2-CWYOajjuE"
      },
      "source": [
        "The same information is plotted, but now it each attribute has a mean of zero and a standard devation of one. The units on the graph are now standard deviaions from the mean.\n",
        "\n",
        "Just to be clear, what we are doing is asking the computer to look for clusters within these 13 students based on two attributes: time spent studying and love of math. We'll see if we can divide them into three clusters. The K-means algorithm we'll be using does this by looking for groups where each member is closer to the average of the group than it is to members of other groups, which is pretty much what we would use as the definition of a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "3AfhBP3ZjjuE"
      },
      "outputs": [],
      "source": [
        "# We will use the sklearn module\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "cols = ['MathLove', 'HoursStudying']\n",
        "data4cluster = data_standardized[cols]\n",
        "kmeans = KMeans(n_init=3, n_clusters=3, random_state=0).fit_predict(data4cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "MT7guJT2jjuE"
      },
      "outputs": [],
      "source": [
        "# The three groups are identified as 0, 1 and 2.\n",
        "data['kmeans']=kmeans\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "KMlJyufcjjuE"
      },
      "outputs": [],
      "source": [
        "# Define colors list, to be used to plot groups: red (=0), green (=1) or blue (=2)\n",
        "colors=['red','green','blue']\n",
        "col = []\n",
        "for ind in kmeans:\n",
        "    col.append(colors[ind])\n",
        "plt.scatter(data['MathLove'], data['HoursStudying'], c=col, s=50)\n",
        "plt.xlabel('Love of Math')\n",
        "plt.ylabel('Hours per Week Spent Studying')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNwJRFYwjjuE"
      },
      "source": [
        "The computer found the expected clusters. The one oddball in the upper left was put in the feels so-so about math and studies a lot, despite having a really strong aversion to math. That is because we told the computer to find exactly three groups, so it had to stick him/her somewhere. This choice of the number of clusters to create is always a question.\n",
        "\n",
        "What if you were told to divide this bunch into exactly two groups?  How would you cluster them? Let's see what the algorithm does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iC0t1NyDjjuE"
      },
      "outputs": [],
      "source": [
        "# Force the data into two clusters\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit_predict(data4cluster)\n",
        "\n",
        "colors=['red','green']\n",
        "col = []\n",
        "for ind in kmeans:\n",
        "    col.append(colors[ind])\n",
        "plt.scatter(data['MathLove'], data['HoursStudying'], c=col, s=50)\n",
        "plt.xlabel('Love of Math')\n",
        "plt.ylabel('Hours per Week Spent Studying')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJzdKualjjuE"
      },
      "source": [
        "For two clusters you should be able to draw a line that divides the two populations. You can see where the line would go in this case, but choise of clusters is not unique. Other divisions are possible.\n",
        "\n",
        "### Take-home Message\n",
        "This example demostrated how you can using a computer algorithm to divided people into different categories based on measured attributes. We did a very simple example with only two attributes, math and studying, but the computer could just as easily have used hundreds of factors. It could have been a lengthy questionaire about your preferences in and ideal mate as part of a dating service, your political views to target you for campaign contributions, or you web browsing history to bombard you with advertisements people in \"your cluster\" are most likely to click on.\n",
        "\n",
        "Many more sophisticated algorithms are continually sifting through your data without human guidance. As you sign away more and more of your personal data in exchange for convenient, free applications, the machines are reading your social media posts, looking at your photos on SnapChat, reviewing your fitness data, monitoring your GPS location, and tracking your browsing history --- and learning all your hopes, fears, and deep, dark secrets. If you listen closely, somewhere in the endless binary stream of ones and zeros you can hear the machine's whispered laughter.\n",
        "\n",
        "![TwitterBot](http://imgs.xkcd.com/comics/twitter_bot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuQjPrBBjjuE"
      },
      "source": [
        "<a id='Student2'></a>\n",
        "## Student Challenge 2\n",
        "[Top of Notebook](#Top)\n",
        "\n",
        "See what happens if you divide the data into four clusters. You will need to add another color to the list to plot the results. [Here is a list](http://matplotlib.org/api/colors_api.html) of the colors that are predefined in matplotlib."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}